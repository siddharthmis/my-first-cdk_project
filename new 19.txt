Certainly! Below is an example of a Lambda function written in Python that sends CloudWatch logs to Splunk using the HTTP Event Collector (HEC) endpoint.

python
Copy code
import boto3
import json
import requests

def send_logs_to_splunk(event, context):
    # Splunk HEC endpoint URL
    splunk_url = "https://<YOUR_SPLUNK_HEC_ENDPOINT_URL>"

    # Splunk HEC token
    splunk_token = "<YOUR_SPLUNK_HEC_TOKEN>"
    
    # Get the CloudWatch log events from the event parameter
    log_events = event["awslogs"]["data"]
    decoded_logs = boto3.client("logs").decode_log_event(splunk_token, log_events)
    
    # Prepare the payload for Splunk
    payload = {
        "event": decoded_logs,
        "sourcetype": "aws:cloudwatchlogs"
    }
    
    # Send the logs to Splunk via HTTP POST request
    response = requests.post(splunk_url, data=json.dumps(payload), headers={"Authorization": "Splunk " + splunk_token})
    
    if response.status_code != 200:
        print("Failed to send logs to Splunk. Response code:", response.status_code)
    else:
        print("Logs successfully sent to Splunk.")
Make sure to replace <YOUR_SPLUNK_HEC_ENDPOINT_URL> with the actual URL of your Splunk HEC endpoint and <YOUR_SPLUNK_HEC_TOKEN> with the Splunk HEC token.

To use this code, you need to create an AWS Lambda function and configure it to trigger on CloudWatch log events. Grant the necessary permissions to the Lambda function to access CloudWatch logs and make HTTP requests.

Note that the example code assumes that your CloudWatch log events are received in the AWS Lambda event parameter in the AWS Logs JSON format. You may need to modify the code if you receive the log events in a different format.





User
Write a Lambda code in Python to send Cloudtrail logs to Splunk
ChatGPT
Certainly! Here's an example of a Lambda function written in Python that sends CloudTrail logs to Splunk using the HTTP Event Collector (HEC) endpoint.

python
Copy code
import json
import boto3
import requests

def send_logs_to_splunk(event, context):
    # Splunk HEC endpoint URL
    splunk_url = "https://<YOUR_SPLUNK_HEC_ENDPOINT_URL>"

    # Splunk HEC token
    splunk_token = "<YOUR_SPLUNK_HEC_TOKEN>"
    
    # Get the CloudTrail log events from the event parameter
    log_events = event["Records"]
    
    # Prepare the payload for Splunk
    payload = {
        "sourcetype": "aws:cloudtrail",
        "event": []
    }
    
    for log_event in log_events:
        # Extract the CloudTrail event from the log event
        cloudtrail_event = json.loads(log_event["Sns"]["Message"])
        
        # Add the CloudTrail event to the payload
        payload["event"].append(cloudtrail_event)
    
    # Send the logs to Splunk via HTTP POST request
    response = requests.post(
        splunk_url,
        data=json.dumps(payload),
        headers={"Authorization": "Splunk " + splunk_token}
    )
    
    if response.status_code != 200:
        print("Failed to send logs to Splunk. Response code:", response.status_code)
    else:
        print("Logs successfully sent to Splunk.")
Make sure to replace <YOUR_SPLUNK_HEC_ENDPOINT_URL> with the actual URL of your Splunk HEC endpoint and <YOUR_SPLUNK_HEC_TOKEN> with the Splunk HEC token.

To use this code, you need to create an AWS Lambda function and configure it to trigger on CloudTrail log events. Grant the necessary permissions to the Lambda function to access CloudTrail logs and make HTTP requests.

Please note that this code assumes the CloudTrail events are received in the AWS Lambda event parameter as SNS records. If you're using a different event source or event format, you may need to modify the code accordingly.





User
Write a Lambda code in Python to send vpc flowlogs logs to Splunk
ChatGPT
Certainly! Here's an example of a Lambda function written in Python that sends VPC Flow Logs to Splunk using the HTTP Event Collector (HEC) endpoint.

python
Copy code
import json
import boto3
import requests

def send_logs_to_splunk(event, context):
    # Splunk HEC endpoint URL
    splunk_url = "https://<YOUR_SPLUNK_HEC_ENDPOINT_URL>"

    # Splunk HEC token
    splunk_token = "<YOUR_SPLUNK_HEC_TOKEN>"
    
    # Get the VPC Flow Logs from the event parameter
    log_events = event["awslogs"]["data"]
    decoded_logs = boto3.client("logs").decode_log_event(log_events)
    
    # Prepare the payload for Splunk
    payload = {
        "event": decoded_logs,
        "sourcetype": "aws:vpcflowlogs"
    }
    
    # Send the logs to Splunk via HTTP POST request
    response = requests.post(
        splunk_url,
        data=json.dumps(payload),
        headers={"Authorization": "Splunk " + splunk_token}
    )
    
    if response.status_code != 200:
        print("Failed to send logs to Splunk. Response code:", response.status_code)
    else:
        print("Logs successfully sent to Splunk.")
Make sure to replace <YOUR_SPLUNK_HEC_ENDPOINT_URL> with the actual URL of your Splunk HEC endpoint and <YOUR_SPLUNK_HEC_TOKEN> with the Splunk HEC token.

To use this code, you need to create an AWS Lambda function and configure it to trigger on VPC Flow Logs. Grant the necessary permissions to the Lambda function to access VPC Flow Logs and make HTTP requests.

Please note that this code assumes the VPC Flow Logs are received in the AWS Lambda event parameter in the AWS Logs JSON format. You may need to modify the code if you receive the log events in a different format.





User
write a bash script to make sure SSM agent is always running and reporting in EC2 linux instance
ChatGPT
Certainly! Here's an example of a bash script that can be used to ensure that the AWS Systems Manager (SSM) agent is always running and reporting on an EC2 Linux instance:

bash
Copy code
#!/bin/bash

# Check if the SSM agent is running
check_ssm_agent() {
    status=$(systemctl is-active amazon-ssm-agent)
    if [[ $status != "active" ]]; then
        return 1
    fi
}

# Start the SSM agent
start_ssm_agent() {
    sudo systemctl start amazon-ssm-agent
    if [[ $? -ne 0 ]]; then
        return 1
    fi
}

# Restart the SSM agent
restart_ssm_agent() {
    sudo systemctl restart amazon-ssm-agent
    if [[ $? -ne 0 ]]; then
        return 1
    fi
}

# Main script logic
if check_ssm_agent; then
    echo "SSM agent is already running."
else
    echo "SSM agent is not running. Starting..."
    if start_ssm_agent; then
        echo "SSM agent started successfully."
    else
        echo "Failed to start SSM agent."
        exit 1
    fi
fi

# Check if the SSM agent is reporting
if ! /usr/bin/amazon-ssm-agent-cli status; then
    echo "SSM agent is not reporting. Restarting..."
    if restart_ssm_agent; then
        echo "SSM agent restarted successfully."
    else
        echo "Failed to restart SSM agent."
        exit 1
    fi
fi

echo "SSM agent is running and reporting."
Save the script in a file (e.g., ssm_agent_check.sh), and make it executable by running chmod +x ssm_agent_check.sh. Then, you can execute the script either manually or set it up as a cron job to run periodically.

The script checks if the SSM agent is running using systemctl and starts it if it's not running. It then uses the amazon-ssm-agent-cli command to check if the agent is reporting. If it's not reporting, the script restarts the SSM agent using systemctl.

Note that the script assumes that the SSM agent is managed using systemctl and the amazon-ssm-agent-cli command is available in the system's PATH. Make sure that the script is executed with sufficient privileges to start and restart the SSM agent.


To reduce the number of legacy kernels from 3 to 2 on RHEL (Red Hat Enterprise Linux) machines, you can follow these steps:

List the installed kernels:

Copy code
rpm -qa kernel
This command will display a list of installed kernels on your system.

Identify the latest two kernels that you want to keep and note down their names.

Remove the extra kernels using the yum command. Replace <KERNEL_PACKAGE_NAME> with the specific package name of the kernel version you want to remove.

arduino
Copy code
sudo yum remove <KERNEL_PACKAGE_NAME>
Repeat step 3 for each of the extra kernels you want to remove.

Update the GRUB configuration to reflect the changes. Run the following command:

bash
Copy code
sudo grub2-mkconfig -o /boot/grub2/grub.cfg
Verify that the kernels have been successfully removed by listing the installed kernels again:

Copy code
rpm -qa kernel
By following these steps, you should be able to reduce the number of legacy kernels from 3 to 2 on your RHEL machines. Remember to exercise caution when removing kernels, as they are critical components of the operating system, and removing the wrong kernel can render your system unusable. It is always recommended to take proper backups and thoroughly test before making any changes to the kernel configuration.








